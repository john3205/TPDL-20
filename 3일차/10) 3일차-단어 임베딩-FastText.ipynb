{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 임베딩 기법 FastText 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 모듈 import 할게요\n",
    "# 거의다 word2vec이랑 동일해요.\n",
    "import random\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 한글을 자모로 나눠서 할거니까\n",
    "# jamo만 하나더 import 할게요.\n",
    "from jamo import h2j, j2h\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Softmax, Activation, Input, Dot, Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas로 챗봇 데이터 읽어 올게요.\n",
    "d_path = '../data/Chatbot_data-master/ChatbotData.csv'\n",
    "df = pd.read_csv(d_path)\n",
    "\n",
    "# 동일하게 컬럼만 한번더 확인할게요\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인할게요.\n",
    "kor_df = df['Q']\n",
    "print(kor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 애초에 50개만 사용할게요.\n",
    "kor_sentences = list(kor_df.values)[:50]\n",
    "kor_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리는 동일해요.\n",
    "okt = Okt()\n",
    "kor_word_splited = [okt.pos(sentence) for sentence in kor_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_set 생성하는 함수 작성할게요.\n",
    "# 미번에는 구두점(\"punctuation\") 제거하도록 할게요.\n",
    "def make_words_set(sentences):\n",
    "    words_set = set()\n",
    "    for sentence in sentences:\n",
    "        for word, pos in sentence:\n",
    "            if pos == \"Punctuation\":\n",
    "                continue\n",
    "#             print(word)\n",
    "            words_set.add(word)\n",
    "    return list(sorted(words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작성한 함수로 words_set 생성할게요.\n",
    "words_set = make_words_set(kor_word_splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words_set 확인해볼게요.\n",
    "print(len(words_set))\n",
    "print(words_set[:5])\n",
    "print(words_set[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어->자음 모음으로 변경하면서\n",
    "# 단어 앞뒤로 특수기호 '<'랑 '>' 넣어주면서\n",
    "# 자음모음을 n-gram 변환하는 함수 작성할게요\n",
    "def word2ngram(word, n):\n",
    "    ngram = []\n",
    "    ws = '<'+h2j(word)+'>'\n",
    "    \n",
    "    for i in range(len(ws)-(n-1)):\n",
    "        ngram.append(ws[i:i+n])\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번 실습에서는 tri-gram만 사용할거에요.\n",
    "# tri_gram_set이랑\n",
    "# 단어: tri_gram Dictionary 생성할게요.\n",
    "tri_gram_set = set()\n",
    "tri_gram = {}\n",
    "\n",
    "for word in words_set:\n",
    "    ngram = word2ngram(word, 3)\n",
    "    tri_gram[word] = ngram\n",
    "    \n",
    "    # set.update()는 ()안에 있는 모든 데이터를\n",
    "    # 한번에 set에 입력해주는 함수에요.\n",
    "    tri_gram_set.update(ngram)\n",
    "    \n",
    "tri_gram_set = list(sorted(tri_gram_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri-gram 딕셔너리 및 길이 확인할게요.\n",
    "pprint(tri_gram)\n",
    "max_len = (max([len(gram) for gram in tri_gram.values()]))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tri-gram set 길이랑 데이터 확인할게요.\n",
    "tri_gram_size = len(tri_gram_set)\n",
    "print(tri_gram_size)\n",
    "print(tri_gram_set[:5])\n",
    "print(tri_gram_set[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 입력, 출력 페어 생성 함수 작성\n",
    "# word2vec이랑 동일해요.\n",
    "# 대신 입력되는 데이터가 단일 문장이에요.\n",
    "def make_pair(sequence, window_size, negative_sample):\n",
    "    x, y = [], []\n",
    "    half_window_size = window_size // 2\n",
    "    \n",
    "    # 이번에는 문장에 정보가 없으면\n",
    "    # Error를 발생시킬게요.\n",
    "    if not sequence:\n",
    "        raise ValueError('No data in sequence')\n",
    "        \n",
    "    for i in range(len(sequence)):\n",
    "        if i < half_window_size:\n",
    "            temp_data = sequence[0:i+half_window_size+1]\n",
    "        elif i == len(sentence):\n",
    "            temp_data = sequence[i-half_window_size:i+1]\n",
    "        else:\n",
    "            temp_data = sequence[i-half_window_size:i+half_window_size+1]\n",
    "\n",
    "        if len(sequence) < window_size:\n",
    "            temp_idx = i\n",
    "        elif len(temp_data) == window_size:\n",
    "            temp_idx = half_window_size\n",
    "        else:\n",
    "            temp_idx = len(temp_data) - (half_window_size+1)\n",
    "        \n",
    "        for idx in range(len(temp_data)):\n",
    "            if idx == temp_idx:\n",
    "                continue\n",
    "            x.append((temp_data[idx], temp_data[temp_idx]))\n",
    "            y.append(1)\n",
    "\n",
    "        for _ in range(negative_sample):\n",
    "            negative_sentence = sequence.copy()\n",
    "            for item in temp_data:\n",
    "                negative_sentence.remove(item)\n",
    "            if not negative_sentence:\n",
    "                continue\n",
    "            random_number = random.randrange(0, len(negative_sentence))\n",
    "            negative_word = negative_sentence[random_number]\n",
    "            for idx in range(len(temp_data)):\n",
    "                if idx == temp_idx:\n",
    "                    continue\n",
    "                x.append((temp_data[idx], negative_word))\n",
    "                y.append(0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 페어 생성 및 확인\n",
    "NX, NY = [], []\n",
    "for idx, sentence in enumerate(kor_sentences):\n",
    "    words = [word for word, _ in okt.pos(sentence) if word in words_set]\n",
    "    # Error 발생 시키려고 임시로 데이터 하나 추가해 볼게요.\n",
    "    words = []\n",
    "    # 확인만 해보시고 지우시거나 주석처리 해주세요.\n",
    "    x, y = make_pair(words, window_size=3, negative_sample=1)\n",
    "    \n",
    "    for word in words:\n",
    "        ngram = tri_gram[word]\n",
    "        n_x, n_y = make_pair(ngram, window_size=3, negative_sample=1)\n",
    "        NX += n_x\n",
    "        NY += n_y\n",
    "        \n",
    "    if idx == 0:\n",
    "        print(NX)\n",
    "        print(NY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정할게요. \n",
    "# 아까랑 거의 비슷해요.\n",
    "vocab_size = len(words_set)\n",
    "tri_gram_size = len(tri_gram_set)\n",
    "embedding_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성하고 summary()로 확인해 볼게요.\n",
    "# 같은데, Embedding Layer에 단어 개수가 아니라\n",
    "# tri_gram의 개수가 들어가요.\n",
    "sub_x_inputs = Input(shape=(1,), dtype='int32')\n",
    "sub_x_embedding = Embedding(tri_gram_size, embedding_size)(sub_x_inputs)\n",
    "\n",
    "sub_y_inputs = Input(shape=(1,), dtype='int32')\n",
    "sub_y_embedding = Embedding(tri_gram_size, embedding_size)(sub_y_inputs)\n",
    "\n",
    "sub_dot_product = Dot(axes=2)([sub_x_embedding, sub_y_embedding])\n",
    "sub_dot_product = Reshape((1,), input_shape=(1, 1))(sub_dot_product)\n",
    "sub_output = Activation('sigmoid')(sub_dot_product)\n",
    "\n",
    "sub_model = Model(inputs=[sub_x_inputs, sub_y_inputs], outputs=sub_output)\n",
    "sub_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 시각화해볼게요.\n",
    "SVG(model_to_dot(sub_model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일할게요.\n",
    "sub_model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 10번만 학습 진행할게요.\n",
    "# 조금 오래 걸려요.\n",
    "for epoch in range(1, 11):\n",
    "    print('Epoch: ', epoch, end=' ')\n",
    "    loss = 0\n",
    "    for x, y in zip(NX, NY):\n",
    "        # 아까는 for문으로 진행했던걸 이번에는\n",
    "        # indexing으로 진행할게요.\n",
    "        x1 = np.asarray(tri_gram_set.index(x[0])).astype('int32').reshape(1, -1)\n",
    "        x2 = np.asarray(tri_gram_set.index(x[1])).astype('int32').reshape(1, -1)\n",
    "        y = np.asarray(y).astype('int32').reshape(1, -1)\n",
    "        loss += sub_model.train_on_batch([x1, x2], y)\n",
    "    print('Loss: ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델의 weight를 불러와서 파일에 쓰도록 할게요.\n",
    "\n",
    "# 파일 생성하고, mode를 쓰기 모드인 'w'로 설정하고\n",
    "# encoding을 'utf-8'로 설정할게요.\n",
    "f = open('sub_fasttext.txt', 'w', encoding='utf-8')\n",
    "\n",
    "# 첫 번째줄 한번 쓰고 넘어갈게요.\n",
    "# 총 tri-gram의 개수랑, embeeding_size를 작성해요.\n",
    "f.write('{} {}\\n'.format(tri_gram_size-1, embedding_size))\n",
    "\n",
    "# 모델에서 weight만 불러 올게요.\n",
    "vectors = sub_model.get_weights()[0]\n",
    "\n",
    "# enumerate() 사용해서 몇 번째인지와 단어를 함께 받아올게요.\n",
    "for i, word in enumerate(tri_gram_set):\n",
    "    \n",
    "    # 첫 줄이랑 동일한 포맷으로 파일에 쓰도록 할게요.\n",
    "    # map(function, values) 함수는 파이썬 내장함수로,\n",
    "    # 모든 value를 function으로 처리한 값을 list로 반환해 줘요.\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "\n",
    "# 마지막에 파일 닫아줄게요.\n",
    "# 안닫으시면 저장이 안되요.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight 작성된 파일을 gensim 모듈로\n",
    "# 읽고 모델로 생성할게요.\n",
    "ft = gensim.models.KeyedVectors.load_word2vec_format('./sub_fasttext.txt', encoding='utf-8', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어는 sum(n-gram[단어]) 이니까\n",
    "# 단어의 벡터로 바꿔주는 생성 함수 작성할게요.\n",
    "def get_word_vector(model, word):\n",
    "    res = []\n",
    "    for gram in word2ngram(word, 3):\n",
    "#         print(gram)\n",
    "        # 학습이 안된 tri_gram은 continue로 \n",
    "        # 넘어가도록 작성할게요.\n",
    "        try:\n",
    "            res.append(model[gram])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return np.sum(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 확인 단어 \"학교\"의 벡터를 확인해볼게요.\n",
    "# 모델 학습할때 tri-gram으로 학습 시켜서\n",
    "# 단어로 입력하면, Error 나요.\n",
    "print(ft['학교'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 확인 단어 \"학교\"의 벡터를 확인해볼게요.\n",
    "# 하까 작성한 함수로 진행할게요.\n",
    "wv_1 = get_word_vector(ft, '학교')\n",
    "print(wv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 확인 단어 \"핵교\"의 벡터를 확인해볼게요.\n",
    "# 아까 작성한 함수로 진행할게요.\n",
    "wv_2 = get_word_vector(ft, '핵교')\n",
    "print(wv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 함수 작성할게요.\n",
    "# -1 ~ 1 값 출력 (-1: 전혀 다름, 1: 완전 동일)\n",
    "def cos_sim(A, B):\n",
    "       return np.dot(A, B)/(np.linalg.norm(A)*np.linalg.norm(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '학교' 과 '핵교'의 유사도 계산 해볼게요.\n",
    "cosim = cos_sim(wv_1, wv_2)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 확인 단어 \"한글\"의 벡터를 확인해볼게요.\n",
    "# 아까 작성한 함수로 진행할게요.\n",
    "wv_3 = get_word_vector(ft, '한글')\n",
    "print(wv_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '학교'과 '한글'의 유사도 계산 해볼게요.\n",
    "cosim = cos_sim(wv_1, wv_3)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 확인 단어 '딥러닝'의 벡터를 확인해볼게요.\n",
    "wv_4 = get_word_vector(ft, '딥러닝')\n",
    "print(wv_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '학교'과 '딥러닝'의 유사도\n",
    "cosim = cos_sim(wv_1, wv_4)\n",
    "print(cosim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 gensim 모듈로 FastText 만들어 볼게요.\n",
    "model = gensim.models.Word2Vec(kor_sentences, size=50, window=3, min_count=1, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kor_sentences = [[word for word, pos in sentence] for sentence in kor_word_splited]\n",
    "print(kor_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.FastText(kor_sentences, size=50, window=3, min_count=1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본 실습에서는 sub_model만 진행했어요.\n",
    "# 전체 모델은 sub_model을 기반으로, \n",
    "# 다시 word2vec 처럼 단어 수준에서 skip-gram을\n",
    "# 진행해요. 그래서 조금 다른값이 나올거에요.\n",
    "model.wv.similarity('한국', '한쿡')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '학교'랑 비슷한 단어를 찾아볼게요.\n",
    "model.wv.most_similar('학교')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
