{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "* TensorFlow나 PyTorch보다 사용자 친화적이다.\n",
    "* 사용하기 쉬운 고수준의 API를 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 주요 Packages\n",
    "* Layers :: 모델을 구현하는데 필요한 레이어를 생성하는 패키지로 **Input(), Dense(), Conv2D(), SimpleRNN()** 등 많은 클래스를 지원  \n",
    "[Layers Documentations:   ](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "\n",
    "* Models :: 구현한 딥러닝 모델을 생성하는 패키지로 **Model(), Sequential()** 클래스를 지원  \n",
    "[Models Documentations:   ](https://www.tensorflow.org/api_docs/python/tf/keras/models)\n",
    "\n",
    "* Losses :: 모델 학습에 필요한 Loss 값을 계산하는 패키지로 **BinaryCrossentropy(), MeanSquaredError()** 등 다양한 클래스를 지원  \n",
    "[Losses Documentations:   ](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n",
    "\n",
    "* Optimizers :: 모델 학습시의 OPtimizers를 생성하는 패키지로 **SGD(), Adam(), RMSprop()** 등 다양한 클래스를 지원  \n",
    "[Optimizers Documentations:   ](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python==3.7  tensorflow==2.2.0 에서 진행하는 실습입니다.\n",
    "# 사용할 모듈 import 하는 부분입니다.\n",
    "\n",
    "# 저희는 tensorflow에 있는 keras를 사용합니다.\n",
    "# 단순히 영어로 번역하시는 겁니다.\n",
    "# tensorflow에서 keras를 import 하겠다.-> from tensorflow import keras\n",
    "from tensorflow import keras ## or\n",
    "\n",
    "# keras 모듈이 크기때문에 특정 레이어나 모델을 import 해서 사용하셔도 됩니다.\n",
    "# tensorflow의 keras의 layers 중에 Input, Dense, Activation... 등을 import 하겠다.\n",
    "# => from tensorflow.keras.layers import Input, Dense, Activation, ....\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, SimpleRNN, LSTM, #... 등\n",
    "\n",
    "# 모델도 동일합니다.\n",
    "# tensorflow의 keras의 models 중에 Model을 import 하겠다.\n",
    "# => from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "# 아래 3줄은 구현한 모델의 시각화를 위한 모듈을 import 하는겁니다.\n",
    "from IPython.display import SVG ## 모델을 시각화 하기위한 모듈\n",
    "from tensorflow.keras.utils import model_to_dot ## 모델을 시각화 하기위한 모듈\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Layers\n",
    "* ***Input()***, ***InputLayer()*** :: 구현할 네트워크의 entry point가 되는 Layer  \n",
    "주로 bold 체의 파라미터들을 건드립니다.\n",
    "\n",
    "\n",
    "**tf.keras.layers.Input** (\n",
    "    **shape=None**, **batch_size=None**, dtype=None, input_tensor=None, sparse=False,\n",
    "    name=None, ragged=False, \\*\\*kwargs\n",
    ")  \n",
    "\n",
    "**tf.keras.layers.InputLayer**(\n",
    "    **input_shape=None**, **batch_size=None**, dtype=None, input_tensor=None, sparse=False,\n",
    "    name=None, ragged=False, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input Layer\n",
    "## Input() 혹은 InputLayer를 통과하게 되면, \n",
    "## 변수의 데이터 타입이 ndarray 등에서 Tensor로 변하게 됩니다.\n",
    "\n",
    "temp_input = np.arange(-5, 6, dtype='float32')\n",
    "print(type(temp_input))\n",
    "print('''----------------------------------------------------------------------''')\n",
    "## Input(), InputLayer() 생성 ##\n",
    "# inp = keras.layers.Input(shape=(6,)) # 입력을 Tensor로 사용 시\n",
    "input_layer = keras.layers.InputLayer(input_shape=(6, )) # 입력을 레이어로 사용 시\n",
    "\n",
    "## InputLayer를 거친 input 데이터\n",
    "print(input_layer(temp_input))\n",
    "print('''----------------------------------------------------------------------''')\n",
    "## InputLayer를 거친 input 데이터의 타입\n",
    "print(type(input_layer(temp_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Dense()*** :: 기본적인 NN Layer  \n",
    "주로 bold 체의 파라미터들을 건드립니다.\n",
    "\n",
    "\n",
    "**tf.keras.layers.Dense** (\n",
    "    **units**, **activation=None**, use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n",
    "    activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n",
    "    \\*\\*kwargs\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dense Layer\n",
    "## Dense Layer는 쉽게 생각하시면, 입력을 units의 개수 바꿔 출력해주는 Layer 입니다.\n",
    "dense_input = np.random.randn(1, 10)\n",
    "\n",
    "## DenseLayer 생성 ::\n",
    "## 본 예시에서 dense_input의 개수는 10개이고, Dense의 units의 개수는 32개 입니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "dense_layer = keras.layers.Dense(units=32)\n",
    "\n",
    "## dense_input을 inputLayer를 거처 DenseLayer에 입력하는 부분입니다.\n",
    "dense_output = dense_layer(input_layer(dense_input))\n",
    "\n",
    "## dense_input의 shape와 dense_output의 shape을 확인하는 부분입니다.\n",
    "## 확인해 보시면 (1, 10)이 (1, 32)로 변경된것을 보실 수 있습니다.\n",
    "print(dense_input.shape)\n",
    "print(dense_output.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Activation()*** :: 이전 레이어의 출력값에 activation 함수를 적용하는 Layer   \n",
    "주로 bold 체의 파라미터들을 건드립니다.\n",
    "\n",
    "\n",
    "**tf.keras.layers.Activation** ( **activation**, \\*\\*kwargs )  \n",
    "\n",
    "activation으로는 주로 'sigmoid', 'tanh', 'relu', 등을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activation Layer\n",
    "activation_input = np.linspace(-1, 1, 10).astype('float')\n",
    "print('Input: \\n', activation_input)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "input_layer = keras.layers.InputLayer()\n",
    "activation_layer_sigmoid = keras.layers.Activation('sigmoid')\n",
    "activation_layer_tanh = keras.layers.Activation('tanh')\n",
    "activation_layer_relu = keras.layers.Activation('relu')\n",
    "activation_layer_softmax = keras.layers.Activation('softmax')\n",
    "\n",
    "sigmoid_output = activation_layer_sigmoid(input_layer(activation_input))\n",
    "print('sigmoid_output: \\n', sigmoid_output)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "tanh_output = activation_layer_tanh(input_layer(activation_input))\n",
    "print('tanh_output: \\n', tanh_output)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "relu_output = activation_layer_relu(input_layer(activation_input))\n",
    "print('relu_output: \\n', relu_output)\n",
    "print('''----------------------------------------------------------------------''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***SimpleRNN()*** :: 가장 기본적인 RNN Layer.  \n",
    "Input_shape은 무조건 3차원으로 이루어 집니다.   \n",
    " EX) (samples, seq_length, features) = (문장의 개수, 문장의 길이(단어의 개수), 단어의 벡터)\n",
    "\n",
    "\n",
    "**tf.keras.layers.SimpleRNN** (\n",
    "    **units**, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='orthogonal', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None,\n",
    "    activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None,\n",
    "    bias_constraint=None, dropout=0.0, recurrent_dropout=0.0,\n",
    "    **return_sequences=False**, **return_state=False**, go_backwards=False, **stateful=False**,\n",
    "    unroll=False, \\*\\*kwargs\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SimpleRNN Layer\n",
    "simpleRNN_input = np.random.randn(2, 5, 10)\n",
    "\n",
    "## SimpleRNN Layer를 생성하는 부분입니다.\n",
    "## Dense와 비슷하게, 입력의 units의 수만큼 바꾸어 출력합니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "simpleRNN_layer = keras.layers.SimpleRNN(units=4)\n",
    "\n",
    "## input과 output을 비교해 보시면, shape이 (2, 5, 10)에서 (2, 4)로 변경된걸\n",
    "## 보실 수 있습니다. 일반적으로 input shape의 제일 앞부분은\n",
    "## 문장의 개수 혹의 이미지의 개수 (sample의 수 혹은 batch의 수)를\n",
    "## 나타내기 때문에 계산에 영향을 받지 않습니다.\n",
    "simpleRNN_output = simpleRNN_layer(input_layer(simpleRNN_input))\n",
    "print(simpleRNN_input.shape)\n",
    "print(simpleRNN_output.shape)\n",
    "print(simpleRNN_output)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "## SimpleRNN Layer에 return_sequences 파라미터를 True로 변경한 레이어입니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "simpleRNN_layer = keras.layers.SimpleRNN(units=4, return_sequences=True)\n",
    "\n",
    "## 입력과 출력의 shape을 확인해 보시면, (2, 5, 10)에서 (2, 5, 4)로 변한것을\n",
    "## 보실 수 있습니다. return_sequences=True가 되면 단어 하나가 들어갈때마다\n",
    "## output을 출력해 주기때문에 문장의 길이(단어의 개수)가 유지됩니다.\n",
    "simpleRNN_output = simpleRNN_layer(input_layer(simpleRNN_input))\n",
    "print(simpleRNN_input.shape)\n",
    "print(simpleRNN_output.shape)\n",
    "print(simpleRNN_output)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "## SimpleRNN Layer에 return_state 파라미터를 True로 변경한 레이어입니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "simpleRNN_layer = keras.layers.SimpleRNN(units=4, return_state=True)\n",
    "\n",
    "## return_state=True가 되면, RNN Layer는 output 뿐만 아니라 hidden_state도\n",
    "## 함께 반환하게 됩니다.\n",
    "simpleRNN_output, state_h = simpleRNN_layer(input_layer(simpleRNN_input))\n",
    "print(simpleRNN_input.shape)\n",
    "print(simpleRNN_output.shape)\n",
    "print(state_h.shape)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "## return_sequences=True와 return_state=True를 함께 사용할 때도 있습니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "simpleRNN_layer = keras.layers.SimpleRNN(units=4, return_sequences=True, return_state=True)\n",
    "simpleRNN_output, state_h = simpleRNN_layer(input_layer(simpleRNN_input))\n",
    "print(simpleRNN_output.shape)\n",
    "print(state_h.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***LSTM()*** :: 메모리 셀이라는 세로운 state가 추가된 RNN => LSTM (Long Short term memory)  \n",
    "**tf.keras.layers.LSTM** (\n",
    "    **units**, activation='tanh', recurrent_activation='sigmoid', use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
    "    bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,\n",
    "    recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
    "    dropout=0.0, recurrent_dropout=0.0, implementation=2, **return_sequences=False**,\n",
    "    **return_state=False**, go_backwards=False, **stateful=False**, time_major=False,\n",
    "    unroll=False, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM Layer\n",
    "## SimpleRNN과 LSTM의 차이점은 두가지 정도로 \n",
    "## 첫번째로 LSTM의 내부구조(계산식)가 SimpleRNN 보다 복잡하다는 점과\n",
    "## 두번째로는 Memory cell 이라는 state가 추가된 점입니다.\n",
    "## 결과적으로는 SimpleRNN과 동일한 output의 shape을 보입니다.\n",
    "LSTM_input = np.random.randn(2, 5, 10)\n",
    "\n",
    "input_layer = keras.layers.InputLayer()\n",
    "LSTM_layer = keras.layers.LSTM(units=4)\n",
    "\n",
    "LSTM_output = LSTM_layer(input_layer(LSTM_input))\n",
    "print(LSTM_output.shape)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "LSTM_layer = keras.layers.LSTM(units=4, return_sequences=True)\n",
    "LSTM_output = LSTM_layer(input_layer(LSTM_input))\n",
    "print(LSTM_output)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "## 유일하게 다른 점은 return_state=True가 되었을때 \n",
    "## return 되는 state가 state_h와 state_c로 2개 라는 점입니다.\n",
    "LSTM_layer = keras.layers.LSTM(units=4, return_state=True)\n",
    "LSTM_output, state_h, state_c = LSTM_layer(input_layer(LSTM_input))\n",
    "print(LSTM_output)\n",
    "print(state_h)\n",
    "print(state_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Bidirectional()*** :: RNN 계열의 레이어에 적용해 양방향으로 반드는 레이어  \n",
    "\n",
    "**tf.keras.layers.Bidirectional**(\n",
    "    **layer**, **merge_mode='concat'**, weights=None, backward_layer=None, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bidirectional Layer\n",
    "LSTM_input = np.random.randn(2, 5, 10)\n",
    "\n",
    "## Bidirectional Layer를 사용하게 되면 RNN계열의 레이어가 정방향 뿐만 아니라 역방향으로도\n",
    "## 계산을 진행하게 됩니다.\n",
    "## 예를들어 \"가 나 다 라\"를 Bidirectional-RNN을 사용해 입력한다면\n",
    "## ['가', '나', '다', '라']와 ['라', '다', '나', '가'] 총 2개를 입력한것과 동일한\n",
    "## 기능을 하게 됩니다.\n",
    "\n",
    "## merge_mode는 두개의 결과를 어떻게 반환할것인지를 결정하는 파라미터입니다.\n",
    "## merge_mode=None 이라면 두개를 따로 반환하며\n",
    "## merge_mode=concat 이라면, 두개를 Concatenate해서 반환합니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "bidirectional_layer = keras.layers.Bidirectional(keras.layers.LSTM(units=4), merge_mode=None)\n",
    "\n",
    "bidirectional_output_forward, bidirectional_output_backward = bidirectional_layer(\n",
    "                                                                input_layer(LSTM_input))\n",
    "print(bidirectional_output_forward)\n",
    "print(bidirectional_output_backward)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "input_layer = keras.layers.InputLayer()\n",
    "bidirectional_layer = keras.layers.Bidirectional(keras.layers.LSTM(4), merge_mode='concat')\n",
    "\n",
    "bidirectional_output = bidirectional_layer(input_layer(LSTM_input))\n",
    "print(bidirectional_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Embedding()*** :: 파라미터 input_dim의 개수만큼 고정된 크기(output_dim)의 벡터를 생성해 index로 접근하는 레이어  \n",
    "\n",
    "**tf.keras.layers.Embedding**(\n",
    "    **input_dim**, **output_dim**, embeddings_initializer='uniform',\n",
    "    embeddings_regularizer=None, activity_regularizer=None,\n",
    "    embeddings_constraint=None, **mask_zero=False**, input_length=None, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding Layer\n",
    "embedding_input = np.arange(0, 6)\n",
    "\n",
    "## Embedding Layer 생성\n",
    "## Embedding Layer에서 input_dim은 몇 개의 벡터가 필요한지를 나타내며, 예를들면 단어의 개수가 됩니다.\n",
    "## output_dim은 벡터를 크기를 몇개로 할것인지를 설정하는 벡터로, 임의로 입력하시면 됩니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "embedding_layer = keras.layers.Embedding(input_dim=len(embedding_input), output_dim=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 입력과 출력을 확인해 보시면, 하나의 인덱스에 고유의 벡터가 생성된것을 볼 수 있습니다.\n",
    "## 입력은 무조건 array로 들어가야 하기 때문에 reshape(-1)을 해줍니다,\n",
    "for input_data in embedding_input:\n",
    "    input_data = input_data.reshape(-1)\n",
    "    print(input_data, \" : \", embedding_layer(input_layer(input_data)))\n",
    "\n",
    "## input_dim 보다 큰값을 입력하면 Error가 발생합니다.\n",
    "# print('6', \" : \", embedding_layer(input_layer(np.asarray([6]))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Dot()*** :: 두 벡터를 축(axse)에 맞춰 내적연산을 하는 레이어  \n",
    "**tf.keras.layers.Dot** ( **axes**, normalize=False, \\*\\*kwargs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dot\n",
    "## Dot Layer는 두 벡터를 내적하는 Layer 입니다.\n",
    "## 자주 쓰는 Layer는 아니기에 존재만 알고 넘어가시면 됩니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "dot_input_1=np.arange(4).reshape(2, 2)\n",
    "dot_input_2=np.arange(4).reshape(2, 2)\n",
    "pprint(dot_input_1)\n",
    "pprint(dot_input_2)\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "dot_ax1 = keras.layers.Dot(axes=1)\n",
    "pprint(dot_ax1([input_layer(dot_inp_1), input_layer(dot_inp_2)]))\n",
    "print('''----------------------------------------------------------------------''')\n",
    "\n",
    "dot_ax2 = keras.layers.Dot(axes=2)\n",
    "dot_inp_1=np.arange(4).reshape(1, 2, 2)\n",
    "dot_inp_2=np.arange(4).reshape(1, 2, 2)\n",
    "pprint(dot_ax2([input_layer(dot_inp_1), input_layer(dot_inp_2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Dropout()*** :: NN의 전체 연결 중 일정 비율(rate)을 제거하는 레이어  \n",
    "**tf.keras.layers.Dropout** ( **rate**, noise_shape=None, seed=None, \\*\\*kwargs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropout\n",
    "dropout_input = np.arange(3).reshape(1, 3)\n",
    "\n",
    "## Dropout Layer를 사용해기 위해 Dense Layer도 함께 생성 해 줍니다.\n",
    "## Dense Layer의 16개 출력 중 25%가 Dropout Layer에 의해 0으로 변경됩니다.\n",
    "input_layer = keras.layers.InputLayer()\n",
    "dense_layer = keras.layers.Dense(16)\n",
    "dropout_layer = keras.layers.Dropout(0.25)\n",
    "\n",
    "dense_output = dense_layer(input_layer(dropout_input))\n",
    "dropout_output = dropout_layer(dense_output, training=True)\n",
    "\n",
    "## Dense Layer의 output과 dropout Layer를 거친 output을 비교해 보시면\n",
    "## 16의 25%인 4개의 출력이 0 혹은 -0으로 변경된것을 보실 수 있습니다. \n",
    "print('Dense_output: \\n', dense_output)\n",
    "print('Dtopout_output: \\n', dropout_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Conv2D()*** :: 2D convolution 연산을 진행하는 레이어 (for CNN)  \n",
    "**tf.keras.layers.Conv2D** (\n",
    "    **filters**, **kernel_size**, **strides=(1, 1)**, **padding='valid'**, data_format=None,\n",
    "    dilation_rate=(1, 1), **activation=None**, use_bias=True,\n",
    "    kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "    kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "    kernel_constraint=None, bias_constraint=None, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D                           # (batch_size, rows, cols, channels)\n",
    "x = np.random.randn(10, 28, 28, 3) # (samples, height, width, features)\n",
    "print(x.shape)\n",
    "# filters: 출력의 개수, kernel_size: kernel의 shape\n",
    "conv2D = keras.layers.Conv2D(filters=4, kernel_size=3, activation='relu')\n",
    "conv_out = conv2D(inp(x))\n",
    "print(conv_out.shape)\n",
    "conv2D = keras.layers.Conv2D(filters=4, kernel_size=3, activation='relu', padding='same')\n",
    "conv_out = conv2D(inp(x))\n",
    "print(conv_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***AveragePooling2D()*** :: 2D에서 pooling size 내부의 값들의 평균을 구하는 레이어 (for CNN)  \n",
    "**tf.keras.layers.AveragePooling2D**(\n",
    "    **pool_size=(2, 2)**, **strides=None**, **padding='valid'**, data_format=None, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# output_shape = (input_shape - pool_size + 1) / strides)\n",
    "x = np.random.randn(10*28*28*3).astype('float').reshape(10, 28, 28, 3)\n",
    "avg_pooling = keras.layers.AveragePooling2D(pool_size=(2, 2), padding='same')\n",
    "avg_out = avg_pooling(inp(x))\n",
    "print(avg_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***MaxPooling2D()*** :: 2D에서 poolling size 내부의 값들 중 최대값을 구하는 레이어 (for CNN)  \n",
    "**tf.keras.layers.MaxPool2D**(\n",
    "    **pool_size=(2, 2)**, **strides=None**, **padding='valid'**, data_format=None, \\*\\*kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_shape = (input_shape - pool_size + 1) / strides)\n",
    "x = np.random.randn(10*28*28*3).astype('float').reshape(10, 28, 28, 3)\n",
    "avg_pooling = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "avg_out = avg_pooling(inp(x))\n",
    "print(avg_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Flatten()*** :: 다차원의 입력을 일차원으로 만드는 레이어  \n",
    "**tf.keras.layers.Flatten** ( data_format=None, \\*\\*kwargs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten\n",
    "x = np.random.randn(3, 28, 28, 3)\n",
    "print(x.shape)\n",
    "flatten = keras.layers.Flatten()\n",
    "print(flatten(inp(x)).shape)\n",
    "print(28*28*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Concatenate()*** :: 두 벡터를 축에 맞춰 하나의 벡터로 묶어주는 레이어  \n",
    "**tf.keras.layers.Concatenate** ( axis=-1, \\*\\*kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "x_1 = np.arange(20).reshape(2, 5, 2)\n",
    "x_2 = np.arange(30). reshape(3, 5, 2)\n",
    "print(x_1.shape)\n",
    "print(x_2.shape)\n",
    "concat = keras.layers.Concatenate(axis=0)\n",
    "print(concat([inp(x_1), inp(x_2)]).shape, '\\n')\n",
    "\n",
    "x_1 = np.arange(20).reshape(2, 2, 5)\n",
    "x_2 = np.arange(40). reshape(2, 4, 5)\n",
    "print(x_1.shape)\n",
    "print(x_2.shape)\n",
    "concat = keras.layers.Concatenate(axis=1)\n",
    "print(concat([inp(x_1), inp(x_2)]).shape, '\\n')\n",
    "\n",
    "x_1 = np.arange(30).reshape(2, 5, 3)\n",
    "x_2 = np.arange(20). reshape(2, 5, 2)\n",
    "print(x_1.shape)\n",
    "print(x_2.shape)\n",
    "concat = keras.layers.Concatenate(axis=2)\n",
    "print(concat([inp(x_1), inp(x_2)]).shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Models\n",
    "* ***Sequential()*** :: 단순히 이전 레이어의 출력이 다음 레이어의 입력이 되는 선형적 흐름 모델 생성에 사용\n",
    "\n",
    "\n",
    "* ***Model()*** :: 선형적 흐름 모델이 아는 경우의 모델 생성에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential()\n",
    "seq_model = keras.models.Sequential()\n",
    "seq_model.add(keras.layers.InputLayer(input_shape=(2,)))\n",
    "seq_model.add(keras.layers.Dense(32, activation='sigmoid'))\n",
    "seq_model.add(keras.layers.Dense(16, activation='relu'))\n",
    "seq_model.add(keras.layers.Dropout(0.2))\n",
    "seq_model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "\n",
    "seq_inp = np.arange(20).reshape(-1, 2)\n",
    "seq_model.predict(seq_inp)\n",
    "\n",
    "%matplotlib inline\n",
    "SVG(model_to_dot(seq_model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model()\n",
    "inp_layer_1 = keras.layers.Input(shape=(2,))\n",
    "inp_layer_2 = keras.layers.Input(shape=(2,))\n",
    "dense_layer_1_1 = keras.layers.Dense(32)\n",
    "dense_layer_1_2 = keras.layers.Dense(8)\n",
    "dense_layer_2 = keras.layers.Dense(4)\n",
    "concat = keras.layers.Concatenate(axis=-1)\n",
    "output_layer = keras.layers.Dense(2, activation='softmax')\n",
    "\n",
    "dense_1_1_out = dense_layer_1_1(inp_layer_1)\n",
    "dense_1_2_out = dense_layer_1_2(inp_layer_2)\n",
    "concat_out = concat([dense_1_1_out, dense_1_2_out])\n",
    "den_2_out = dense_layer_2(concat_out)\n",
    "output = output_layer(den_2_out)\n",
    "\n",
    "model = keras.models.Model([inp_layer_1, inp_layer_2], output)\n",
    "\n",
    "%matplotlib inline\n",
    "SVG(model_to_dot(model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Loss & Optimizer\n",
    "#### 자주 사용되는 Losses\n",
    "* ***BinaryCrossentropy()*** :: 데이터를 True/False로 나눌때 사용되는 손실함수, 'binary_crossentropy'\n",
    "* ***CategoricalCrossentropy()*** :: 데이터를 다수의 카테고리로 나눌때 사용되는 손실함수, 'categorical_crossentropy'\n",
    "* ***CosineSimilarity()*** :: 데이터간의 코사인 유사도를 Loss로 사용하는 손실함수, 'cosine_similarity'\n",
    "* ***MeanSquaredError()***  :: square(y_true - y_pred)를 Loss로 사용하는 손실함수, 'mean_squared_error', 'mse'\n",
    "\n",
    "#### 자주 사용되는 Optimizers\n",
    "* ***SGD()***, 'SGD'\n",
    "* ***Adam()***, 'Adam'\n",
    "* ***RMSprop()***, 'RMSprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용법\n",
    "# model.compile(loss='binary_crossentropy', optimizer='SGD')\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='Adam')\n",
    "model.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
    "## 필요에 따라 셋 중 하나만 사용하시는 겁니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현하기 문제 1. \n",
    "1. VGG16 모델을 만들어 보세요. (CNN 모델)\n",
    "* VGG16 Model\n",
    "![VGG16](./img/vgg16_real.png)\n",
    "\n",
    "\n",
    "* VGG16 Archtecture  \n",
    "![VGG16](./img/vgg16_sim_arch.png)\n",
    "\n",
    "이미지 출처: https://neurohive.io/en/popular-networks/vgg16/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=(224,224,3)))\n",
    "model.add(keras.layers.Conv2D(filters=64//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=64//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=128//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=128//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=256//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=256//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=256//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=512//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=512//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=512//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(keras.layers.Conv2D(filters=512//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=512//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.Conv2D(filters=512//2, kernel_size=3, activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D())\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(7*7*512//2, activation='relu'))\n",
    "model.add(keras.layers.Dense(7*7*512//2, activation='relu'))\n",
    "model.add(keras.layers.Dense(7*7*512//2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1000, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "## SVG는 jupyter notebook에 바로 모델을 그려주는 모듈입니다.\n",
    "SVG(model_to_dot(model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))\n",
    "\n",
    "## plot_model은 to_file='file_name'으로 모델의 그림을 저장해주는 모듈입니다.\n",
    "# plot_model(model, to_file='./vgg_model.png', show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구현하기 문제 2.\n",
    "2. Google Inception v3 모델 중 제일 마지막에 있는 Inception Module C를 만들어 보세요. (CNN 모델)\n",
    "3. Inception Module C 다음의 출력층을 만들고 연결해보세요.\n",
    "\n",
    "\n",
    "* Input의 shape은 (8, 8, 1280)\n",
    "* Inception Module C 내부의 filters = {1: 768, 3:(384, 768), 5:(96, 256), pool:256}\n",
    "\n",
    "\n",
    "* Inception V3 Full Architecture\n",
    "![InceptionV3 Archi](./img/InceptionV3_Architecture.png)\n",
    "------------------------\n",
    "* Inception Module C (왼쪽부터 5, 3, pool, 1)\n",
    "![IMC](./img/IMC.png)\n",
    "\n",
    "이미지 출처(Architecture): https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c  \n",
    "\n",
    "이미지 출처(Inception Module C): https://datascienceschool.net/view-notebook/8d34d65bcced42ef84996b5d56321ba9/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module_c(x):\n",
    "    # 1, 3, 5, pool\n",
    "    # 384, (192, 384), (48, 128), 128\n",
    "    \n",
    "    ## inception_module_c의 레이어를 생성하는 부분입니다.\n",
    "    \n",
    "    ## 5, 그림에서 제일 왼쪽 부분입니다.\n",
    "    conv_01 = keras.layers.Conv2D(filters=48*2, kernel_size=(1, 1),\n",
    "                                  activation='relu', padding='same')\n",
    "    conv_02 = keras.layers.Conv2D(filters=128*2, kernel_size=(3, 3),\n",
    "                                  activation='relu', padding='same')\n",
    "    conv_03 = keras.layers.Conv2D(filters=128*2, kernel_size=(1, 3),\n",
    "                                  activation='relu', padding='same')\n",
    "    conv_04 = keras.layers.Conv2D(filters=128*2, kernel_size=(3, 1),\n",
    "                                  activation='relu', padding='same')\n",
    "    \n",
    "    ## 3, 그림에서 중간 왼쪽 부분입니다.\n",
    "    conv_11 = keras.layers.Conv2D(filters=192*2, kernel_size=(1, 1),\n",
    "                                  activation='relu', padding='same')\n",
    "    conv_12 = keras.layers.Conv2D(filters=384*2, kernel_size=(1, 3),\n",
    "                                  activation='relu', padding='same')\n",
    "    conv_13 = keras.layers.Conv2D(filters=384*2, kernel_size=(3, 1),\n",
    "                                  activation='relu', padding='same')\n",
    "    \n",
    "    ## Pool, 그림에서 중간 오른쪽 부분입니다.\n",
    "    avg_pool_21 = keras.layers.AveragePooling2D(pool_size=(3, 3), strides=1,\n",
    "                                                padding='same')\n",
    "    conv_22 = keras.layers.Conv2D(filters=128*2, kernel_size=(1, 1),\n",
    "                                  activation='relu', padding='same')\n",
    "    \n",
    "    ## 1, 그림에서 제일 오른쪽 부분입니다.\n",
    "    conv_31 = keras.layers.Conv2D(filters=384*2, kernel_size=(1, 1),\n",
    "                                  activation='relu', padding='same')\n",
    "    \n",
    "    ## out\n",
    "    concat = keras.layers.Concatenate(axis=-1)\n",
    "    \n",
    "    ## inception_module_c의 레이어를 연결해 주는 부분입니다.\n",
    "    x0_0 = conv_04(conv_02(conv_01(x)))\n",
    "    x0_1 = conv_03(conv_02(conv_01(x)))\n",
    "    x1_0 = conv_13(conv_11(x))\n",
    "    x1_1 = conv_12(conv_11(x))\n",
    "    x2 = conv_22(avg_pool_21(x))\n",
    "    x3 = conv_31(x)\n",
    "    out = concat([x0_0, x0_1, x1_0, x1_1, x2, x3])\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "## 출력층 레이어 생성부분 입니다.\n",
    "avg_pool = keras.layers.AveragePooling2D(pool_size=(7, 7))\n",
    "dense_fc = keras.layers.Dense(1000)\n",
    "dense_out = keras.layers.Dense(1000, activation='softmax')\n",
    "\n",
    "\n",
    "## 전체 레이어를 연결해 주는 부분입니다.\n",
    "input_layer = keras.layers.Input(shape=(8, 8, 1280))\n",
    "imc_out_01 = inception_module_c(input_layer)\n",
    "imc_out_02 = inception_module_c(imc_out_01)\n",
    "output = dense_out(dense_fc(avg_pool(imc_out_02)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(input_layer, output)\n",
    "SVG(model_to_dot(model, show_shapes=True, dpi=50).create(prog='dot', format='svg'))\n",
    "# plot_model(model, to_file='./imc_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
