{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 생성 - 한글->영어 기계 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 모듈 import\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1_구어체_190920.xlsx', '2_대화체_190920.xlsx', '3_문어체_뉴스_190920.xlsx', '4_문어체_한국문화_190920.xlsx', '5_문어체_조례_190920.xlsx', '6_문어체_지자체웹사이트_190920.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# 데이터가 있는 폴더 위치 설정 \n",
    "d_path = '../data/KEnglish_Text_Corpus_sample/'\n",
    "\n",
    "# 내부 파일 확인하기\n",
    "files = os.listdir(d_path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temp                               한국어  \\\n",
      "0     15005  나는 네가 한국 언어를 배워서 한국어로 대화했으면 좋겠어.   \n",
      "1     15006  나는 대략 한 시간 정도 이 창고를 돌아 보기를 원합니다.   \n",
      "2     15007   때를 미는 한국의 목욕법을 체험해보고 싶은데 가능할까요?   \n",
      "3     15008    우리들 또한 당신들과 함께 한다면 큰 영광일 것입니다.   \n",
      "4     15016      나는 네가 한국의 감탄사를 사용한다는 말에 놀랐다.   \n",
      "...     ...                               ...   \n",
      "9994   2229           아침에 물 흐르는 좋은 소리를 들었습니다.   \n",
      "9995   2240         Anne은 여름 휴가를 어떻게 보낼 것인가요?   \n",
      "9996   2242            이번 여름 휴가는 일본에 가려고 했어요.   \n",
      "9997   2243              설 연휴 휴무 일정이 어떻게 되나요?   \n",
      "9998   2251           나는 올해 휴가 때 사이판을 다녀왔습니다.   \n",
      "\n",
      "                                                     영어  \\\n",
      "0     I want you to learn Korean and talk with me in...   \n",
      "1     I want to look around this warehouse for about...   \n",
      "2     I want to experience Korean skin peeling, but ...   \n",
      "3     It will be truly honored for us to work with you.   \n",
      "4     I was surprised that you used Korea's exclamat...   \n",
      "...                                                 ...   \n",
      "9994       I heard the water is running in the morning.   \n",
      "9995           How will Anne spend the summer vacation?   \n",
      "9996   I have planned to go Japan this summer vacation.   \n",
      "9997         What are the New Year’s holiday schedules?   \n",
      "9998    I went to Saipan during the vacation this year.   \n",
      "\n",
      "                                                  영어 검수  \n",
      "0     I want you to learn Korean so that we can talk...  \n",
      "1     I want to look around this warehouse for about...  \n",
      "2     I want to experience a Korean skin peeling bat...  \n",
      "3     It will be truly honored for us to work with you.  \n",
      "4     I was surprised to hear that you are using Kor...  \n",
      "...                                                 ...  \n",
      "9994            I heard the water flows in the morning.  \n",
      "9995       What is Anne's plan for the summer vacation?  \n",
      "9996  I had a plan to go to Japan this summer vacation.  \n",
      "9997                    When is the New Year’s holiday?  \n",
      "9998             I went to Saipan during this vacation.  \n",
      "\n",
      "[9999 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1_구어체_190920.xlsx 데이터 \n",
    "# pandas로 읽어서 DataFrame 생성하기\n",
    "# 및 확인하기\n",
    "xlsx = pd.read_excel(d_path+files[0])\n",
    "print(xlsx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       나는 네가 한국 언어를 배워서 한국어로 대화했으면 좋겠어.\n",
      "1       나는 대략 한 시간 정도 이 창고를 돌아 보기를 원합니다.\n",
      "2        때를 미는 한국의 목욕법을 체험해보고 싶은데 가능할까요?\n",
      "3         우리들 또한 당신들과 함께 한다면 큰 영광일 것입니다.\n",
      "4           나는 네가 한국의 감탄사를 사용한다는 말에 놀랐다.\n",
      "                      ...               \n",
      "9994             아침에 물 흐르는 좋은 소리를 들었습니다.\n",
      "9995           Anne은 여름 휴가를 어떻게 보낼 것인가요?\n",
      "9996              이번 여름 휴가는 일본에 가려고 했어요.\n",
      "9997                설 연휴 휴무 일정이 어떻게 되나요?\n",
      "9998             나는 올해 휴가 때 사이판을 다녀왔습니다.\n",
      "Name: 한국어, Length: 9999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# DataFrame에서 한국어만 확인하기\n",
    "print(xlsx['한국어'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I want you to learn Korean and talk with me in...\n",
      "1       I want to look around this warehouse for about...\n",
      "2       I want to experience Korean skin peeling, but ...\n",
      "3       It will be truly honored for us to work with you.\n",
      "4       I was surprised that you used Korea's exclamat...\n",
      "                              ...                        \n",
      "9994         I heard the water is running in the morning.\n",
      "9995             How will Anne spend the summer vacation?\n",
      "9996     I have planned to go Japan this summer vacation.\n",
      "9997           What are the New Year’s holiday schedules?\n",
      "9998      I went to Saipan during the vacation this year.\n",
      "Name: 영어, Length: 9999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# DataFrame에서 영어만 확인하기\n",
    "print(xlsx['영어'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "array(['나는 네가 한국 언어를 배워서 한국어로 대화했으면 좋겠어.',\n",
      "       '나는 대략 한 시간 정도 이 창고를 돌아 보기를 원합니다.',\n",
      "       '때를 미는 한국의 목욕법을 체험해보고 싶은데 가능할까요?',\n",
      "       '우리들 또한 당신들과 함께 한다면 큰 영광일 것입니다.', '나는 네가 한국의 감탄사를 사용한다는 말에 놀랐다.'],\n",
      "      dtype=object)\n"
     ]
    }
   ],
   "source": [
    "# DataFrame에서 한국어의 values만 추출하기(문장만 추출)\n",
    "kor_sentences = xlsx['한국어'].values\n",
    "\n",
    "# 한국어 문장의 개수 확인하기\n",
    "print(len(kor_sentences))\n",
    "\n",
    "# 한국어 문장 앞에서 5개 확인하기\n",
    "pprint(kor_sentences[:5])\n",
    "\n",
    "# 사용할 한국어 문장 개수를 100개로 줄이기\n",
    "kor_sentences = kor_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n",
      "array(['I want you to learn Korean and talk with me in Korean.',\n",
      "       'I want to look around this warehouse for about an hour.',\n",
      "       'I want to experience Korean skin peeling, but would it be possible?',\n",
      "       'It will be truly honored for us to work with you.',\n",
      "       \"I was surprised that you used Korea's exclamation.\"], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "# DataFrame에서 영어의 values만 추출하기(문장만 추출)\n",
    "eng_sentences = xlsx['영어'].values\n",
    "\n",
    "# 영어 문장의 개수 확인하기\n",
    "print(len(eng_sentences))\n",
    "\n",
    "# 영어 문장 앞에서 5개 확인하기\n",
    "pprint(eng_sentences[:5])\n",
    "\n",
    "# 사용할 영어 문장 개수 100개로 줄이기\n",
    "eng_sentences = eng_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 words_set(단어의 집합) 생성\n",
    "# 및 한글 문장의 최대길이 확인하기\n",
    "\n",
    "# set 변수 생성\n",
    "kor_words_set = set()\n",
    "\n",
    "# 문장 최대길이 변수 생성\n",
    "kor_max_len = 0\n",
    "\n",
    "# enemerate(kor_sentences)로 한국어 문장과 문장의 위치를 함깨 받기\n",
    "for ix, sentence in enumerate(kor_sentences):\n",
    "    \n",
    "    # 문장의 죄우 공백 및 줄바꿈 문자를 제거하고 띄워쓰기로 단어를 나눠서 \n",
    "    # 각 단어에 다시 공백을 제거해 준 뒤 words에 저장\n",
    "    words = [word.strip() for word in sentence.strip().split(' ')]\n",
    "    \n",
    "    # 한글 문장의 최대길이 업데이트\n",
    "    if len(words) > kor_max_len:\n",
    "        kor_max_len = len(words)\n",
    "    \n",
    "    # words에 빈 데이터가 들어있으면 제거\n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "        \n",
    "    # 한글 단어 집합에 현재 단어들 추가\n",
    "    kor_words_set.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#kor words:  584\n",
      "max seq length:  12\n"
     ]
    }
   ],
   "source": [
    "# 한글 words_set 확인\n",
    "\n",
    "# 단어 집합을 정렬한 뒤 타입을 리스트로 변경\n",
    "kor_words_set = list(sorted(kor_words_set))\n",
    "\n",
    "# zero-padding을 위해서 한글 단어 집합의 0번째에 공백 추가\n",
    "kor_words_set.insert(0, '')\n",
    "\n",
    "# 한글 단어 개수 확인\n",
    "print('#kor words: ', len(kor_words_set))\n",
    "\n",
    "# 한글 문장의 최대길이 확인\n",
    "print('max seq length: ', kor_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 words_set(단어 집합) 생성\n",
    "# 및 영어 최대 길이 문장 설정\n",
    "\n",
    "# 영어 단어 집합 변수 생성\n",
    "eng_words_set = set()\n",
    "\n",
    "# 영어 최대 길이 변수 생성\n",
    "eng_max_len = 0\n",
    "\n",
    "# 영어 문장과 해당 문장이 몇 번째 문장인지 함께 받기\n",
    "for ix, sentence in enumerate(eng_sentences):\n",
    "    \n",
    "    # 문장의 죄우 공백 및 줄바꿈 문자를 제거하고 띄워쓰기로 단어를 나눠서 \n",
    "    # 각 단어에 다시 공백을 제거해 준 뒤 words에 저장\n",
    "    words = [word.strip() for word in sentence.strip().split(' ')]\n",
    "    \n",
    "    # 영어 문장 최대길이 업데이트\n",
    "    if len(words) > eng_max_len:\n",
    "        eng_max_len = len(words)\n",
    "        \n",
    "    # words에 빈 데이터가 있으면 제거 \n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "    \n",
    "    # 영어 단어 집합에 현재 단어들 추가\n",
    "    eng_words_set.update(words)\n",
    "    \n",
    "# 영어 문장의 시작을 알릴 Special Symbol인 '<start>'와\n",
    "# 끝을 알릴 pecial Symbol '<eod>'를 영어 단어 집합에 추가\n",
    "eng_words_set.update(['<start>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#end words 505\n",
      "max seq length:  18\n"
     ]
    }
   ],
   "source": [
    "# 영어 words_set 확인\n",
    "\n",
    "# 영어 단어 집합을 정렬하고, 리스트로 타입 변경\n",
    "eng_words_set = list(sorted(eng_words_set))\n",
    "\n",
    "# zero-padding을 위해서, 영어 단어 집합의 0번째에\n",
    "# 빈 데이터 추가\n",
    "eng_words_set.insert(0, '')\n",
    "\n",
    "# 영어 집합에 있는 단어의 개수 확인\n",
    "print('#end words', len(eng_words_set))\n",
    "\n",
    "# 영어 문장의 최대 길이 확인\n",
    "print('max seq length: ', eng_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 문장 정수 인코딩 및 패딩 함수 작성\n",
    "\n",
    "# 하나의 문장이 입력되면,\n",
    "# 인코딩 및 패딩을 진행한 데이터로 변경 후 반환\n",
    "def kor_encoding(sentence):\n",
    "    \n",
    "    # 문장에서 공백 및 줄바꿈 문자를 제거하고\n",
    "    # ' '(띄어쓰기)로 문장을 잘라 단어로 만든다음\n",
    "    # 단어에서 다시 공백 및 줄바꿈 문자를 제거하고\n",
    "    # words에 저장\n",
    "    words = [word.strip() for word in sentence.strip().split(' ')]\n",
    "    \n",
    "    # words에서 빈 데이터 제거\n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "\n",
    "    # words에 들어있는 단어를 한국어 단어 집합에서 찾아\n",
    "    # 그 위치로 정수 인코딩 진행\n",
    "    words = [kor_words_set.index(word) for word in words]\n",
    "    \n",
    "    # 함수의 반환 변수 생성 및 데이터 할당\n",
    "    encode_sentence = words\n",
    "    \n",
    "    # 최대 문장의 길이와 동일하게 맞춰야 하므로\n",
    "    # 최대 문장의 길이에서 현재 문장의 길이를 뺀 수 만큼\n",
    "    # zero-padding을 진행\n",
    "    for _ in range(kor_max_len-len(encode_sentence)):\n",
    "        encode_sentence.insert(0, 0)\n",
    "    \n",
    "    # zero-padding이 진행된 변수를 np.ndarray 타입으로 변경하고,\n",
    "    # 데이터를 float타입으로 변경한 후 반환\n",
    "    return np.asarray(encode_sentence).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.  94. 114. 533. 335. 232. 538. 142. 441.]\n",
      " [  0.   0.  94. 136. 530. 297. 428. 376. 477. 152. 243. 368.]\n",
      " [  0.   0.   0.   0.   0. 170. 224. 543. 210. 486. 311.   8.]\n",
      " [  0.   0.   0.   0. 363. 172. 130. 556. 546. 507. 343.  45.]\n",
      " [  0.   0.   0.   0.   0.  94. 114. 543.  20. 269. 190. 116.]]\n"
     ]
    }
   ],
   "source": [
    "# 한국어 정수 인코딩 및 확인\n",
    "\n",
    "# 정수 인코딩이 진행된 모든 문장을 저장할 변수를 list로 생성\n",
    "encode_sentences = []\n",
    "\n",
    "# 모든 한국어 문장을 한번씩 반복하면서 진행\n",
    "for sentence in kor_sentences:\n",
    "    # 작성한 함수를 이용해 문장을 인코딩 하고\n",
    "    # 위에서 생성한 list에 추가.\n",
    "    encode_sentences.append(kor_encoding(sentence))\n",
    "\n",
    "# 변수의 타입을 np.ndarray로 변경하고,\n",
    "# 데이터의 타입을 float으로 변경.\n",
    "encode_sentences = np.asarray(encode_sentences).astype('float')\n",
    "\n",
    "# 인코딩된 한국어 문장 앞에서 5개 확인\n",
    "print(encode_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어 문장 Special Symbol 포함 정수 인코딩 및 패딩\n",
    "\n",
    "# 정수 인코딩이 진행된 모든 영어 문장을 저장할 변수를 list로 생성\n",
    "decode_sentences = []\n",
    "\n",
    "# 모든 영어 문장을 한번씩 반복하면서 진행\n",
    "for sentence in eng_sentences:\n",
    "    \n",
    "    # 영어 문장의 좌우 공백 및 줄바꿈 문자를 제거하고, 띄워쓰기로 나눈 후\n",
    "    # 나눠진 단어에 다시 좌우 공백 및 줄바꿈 문자를 제거하고 words에 저장\n",
    "    words = [word.strip() for word in sentence.strip().split(' ')]\n",
    "    \n",
    "    # words에서 빈 데이터 제거\n",
    "    while '' in words:\n",
    "        words.remove('')\n",
    "    \n",
    "    # words에 들어있는 영어 단어를 영어 단어 집합에서 찾아\n",
    "    # 그 위치로 정수 인코딩\n",
    "    words = [eng_words_set.index(word) for word in words]\n",
    "    \n",
    "    # zero-padding 조작을 위한 변수 생성 및 할당\n",
    "    decode_sentence = words\n",
    "    \n",
    "    # 문장의 최대 길이와 동일해 지도록 zero-padding을 진행해야 하므로,\n",
    "    # 영어 문장 최대길이와 현재 문장이 길이의 차이만큼 반복하면서\n",
    "    # 0을 앞쪽에 축가\n",
    "    for _ in range(eng_max_len-len(decode_sentence)):\n",
    "        decode_sentence.insert(0, 0)\n",
    "        \n",
    "    # bug_line: sentence의 0번째 단어를 바꿀 시, 최대 길이의 문장에서\n",
    "    #           첫번째 단어가 사라짐.\n",
    "    # line의 목적: zero-padding이 진행된 영어 문장의 앞에 \n",
    "    #              문장의 시작을 알리는 Special Symbol인 '<start>'의 위치 추가\n",
    "    decode_sentence[0]=eng_words_set.index('<start>')\n",
    "#    아래 문장으로 변경\n",
    "#    decode_sentence.insert(0, eng_words_set.index('<start>'))\n",
    "    \n",
    "    # zero-padding이 진행된 영어 문장의 끝에\n",
    "    # 문장의 끝을 알리는 Special Symbol인 '<eos>'의 위치 추가\n",
    "    decode_sentence.append(eng_words_set.index('<eos>'))\n",
    "    \n",
    "    # 위에서 만든 list에 인코딩된 문장 저장\n",
    "    decode_sentences.append(decode_sentence)\n",
    "\n",
    "# 변수의 타입을 np.ndarray로 변경 및\n",
    "# 데이터의 타입을 float으로 변경\n",
    "decode_sentences = np.asarray(decode_sentences).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.   0.   0.   0.   0.   0.  34. 475. 502. 447. 270.  49.  99. 427.\n",
      "  492. 293. 252.  50.   3.]\n",
      " [  4.   0.   0.   0.   0.   0.   0.  34. 475. 447. 282. 104. 442. 476.\n",
      "  207.  82.  98. 245.   3.]\n",
      " [  4.   0.   0.   0.   0.   0.  34. 475. 447. 192.  49. 400. 335. 129.\n",
      "  498. 264. 113. 354.   3.]\n",
      " [  4.   0.   0.   0.   0.   0.   0.  41. 491. 113. 461. 242. 207. 469.\n",
      "  447. 495. 492. 503.   3.]\n",
      " [  4.   0.   0.   0.   0.   0.   0.   0.   0.   0.  34. 477. 422. 432.\n",
      "  502. 470.  45. 191.   3.]]\n"
     ]
    }
   ],
   "source": [
    "# 인코딩된 영어 문장의 앞에서 5개 확인\n",
    "print(decode_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 데이터를 One hot 인코딩\n",
    "\n",
    "# 한국어 문장을 One hot 인코딩\n",
    "encoder_train = to_categorical(encode_sentences)\n",
    "\n",
    "# 모든 영어 문장에서 제일 마지막 단어('<eos>')를 제외하고\n",
    "# One hot 인코딩\n",
    "decoder_train = to_categorical(decode_sentences[:, :-1])\n",
    "\n",
    "# 모든 영어 문장에서 제일 처음 단어('<start>')를 제외하고\n",
    "# One hot 인코딩\n",
    "decoder_target = to_categorical(decode_sentences[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 12, 584)\n",
      "(100, 18, 505)\n",
      "(100, 18, 505)\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "[[[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 각각의 데이터 shape 및 앞에서 2개 확인\n",
    "print(encoder_train.shape)\n",
    "print(decoder_target.shape)\n",
    "print(decoder_train.shape)\n",
    "print(encoder_train[:2])\n",
    "print(decoder_train[:2])\n",
    "print(decoder_target[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 저장을 위한 클래스 생성\n",
    "# 기존 Class의 함수를 overriding 하기위해\n",
    "# keras.callbacks.Callback를 상속받는 클래스\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    \n",
    "    # self.losses를 생성 및 초기화 하는\n",
    "    # 초기화 함수\n",
    "    def init(self):\n",
    "        self.losses = []\n",
    "        \n",
    "    # 기존 클래스의 함수 overriding \n",
    "    # 학습 진행도중 loss만 가지고 와서\n",
    "    # 클래스 변수인 self.losses에 저장\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN units의 개수 설정\n",
    "num_units = 128\n",
    "# 한글 단어(토큰) 개수 설정\n",
    "kor_num_token = len(kor_words_set)\n",
    "\n",
    "# 영어 단어(토큰) 개수 설정\n",
    "eng_num_token = len(eng_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 레이어 생성 및 연결\n",
    "\n",
    "# 인코더 인풋 레이어 shape=(문장 개수, 토큰 개수) 생성\n",
    "# 및 레이어의 이름을 encoder_input으로 설정\n",
    "# 문장의 개수는 batch_size로 인식되어 None 가능\n",
    "enc_inputs = Input(shape=(None, kor_num_token), name='encoder_input')\n",
    "\n",
    "# 인코더 SimpleRNN 레이어 생성\n",
    "# units=num_units, return_sequences=True, return_state=True, 및 이름을 encoder로 설정\n",
    "# return_sequences=True 설정 시 모든 unit에서 출력이 발생\n",
    "# return_state=True 설정 시, 마지막 unit에서 생성된 state를 반환\n",
    "enc = SimpleRNN(num_units, return_sequences=True, return_state=True, name='encoder')\n",
    "\n",
    "# 인풋 레이어와 SimpleRNN 레이어를 연결해 인코더를 생성하며,\n",
    "# SimpleRNN의 출력과, state를 받음\n",
    "enc_outputs, enc_state = enc(enc_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 레이어 생성 및 연결\n",
    "\n",
    "# 디코더 인풋 레이어 shape=(문장 개수, 토큰 개수) 생성\n",
    "# 및 레이어의 이름을 decoder_input으로 설정\n",
    "# 문장의 개수는 batch_size로 인식되어 None 가능\n",
    "dec_inputs = Input(shape=(None, eng_num_token), name='decoder_input')\n",
    "\n",
    "# 디코더 SimpleRNN 레이어 생성\n",
    "# units=num_units, return_sequences=True, return_state=True, 및 이름을 decoder로 설정\n",
    "# return_sequences=True 설정 시 모든 unit에서 출력이 발생\n",
    "# return_state=True 설정 시, 마지막 unit에서 생성된 state를 반환\n",
    "dec = SimpleRNN(num_units, return_sequences=True, return_state=True, name='decoder')\n",
    "\n",
    "# 인풋 레이어와 SimpleRNN 레이어를 연결해 디코더를 생성하며,\n",
    "# SimpleRNN의 출력과, state를 받되, 학습에는 디코더의 state를 사용하지 않음\n",
    "dec_outputs, _ = dec(dec_inputs, initial_state=enc_state)\n",
    "\n",
    "# 디코더의 출력을 조정할 Dense레이어 생성\n",
    "# 출력의 개수를 영어 단어(토큰)의 개수로 맞추고, \n",
    "# 활성화 함수를 softmax로 지정 후 이름을 decoder_output으로 설정\n",
    "dec_dense = Dense(eng_num_token, activation='softmax', name='decoder_output')\n",
    "\n",
    "# 디코더와 Dense레이어 연결\n",
    "dec_outputs = dec_dense(dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "# 입력 = [인코더 인풋 레이어, 디코더 인풋 레이어]\n",
    "# 출력 = 디코더의 출력\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "# loss함수는 categorical_crossentropy\n",
    "# optimizer는 rmsprop 사용\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.9922\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1434 - accuracy: 0.9956\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1356 - accuracy: 0.9933\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1264 - accuracy: 0.9967\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9944\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1063 - accuracy: 0.9983\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0988 - accuracy: 0.9967\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.1012 - accuracy: 0.9944\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0892 - accuracy: 0.9950\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0768 - accuracy: 0.9989\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0694 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0707 - accuracy: 0.9989\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0591 - accuracy: 0.9989\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0650 - accuracy: 0.9972\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0516 - accuracy: 0.9994\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0432 - accuracy: 0.9994\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9994\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0341 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0342 - accuracy: 0.9994\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0265 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0155 - accuracy: 0.9994\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0136 - accuracy: 0.9989\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0118 - accuracy: 0.9983\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.7659e-04 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 8.2737e-04 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.4263e-04 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 0.9989\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.2862e-04 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.4050e-04 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.6075e-04 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.0146e-04 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.6955e-04 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.0957e-04 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.6625e-04 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1353e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.6433e-04 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2756e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.9903e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.8590e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.0837e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2991e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.7346e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4848e-04 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.2924e-04 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.1434e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.0285e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.0605e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7815e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.1521e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.8723e-04 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 0.9994\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.0291e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.1220e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 9.0533e-05 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 7.5538e-05 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 6.4646e-05 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.5849e-05 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9708e-05 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9472e-05 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 0.0015 - accuracy: 0.9994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25a3fea9f88>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "# 모델을 생성할때 입력한 입력과 출력의 형태를 그대로 사용\n",
    "# 인코더 인풋 = zero-padding이 진행된 한국어 문장 \n",
    "# 디코더 인풋 = zero-padding이 진행된 영어 문장 중 마지막 단어('<eos>')를 제거한 문장\n",
    "# 디코더 아웃풋 = zero-papdding이 진행된 영어 문장 중 처음 단어('<start>')를 제거한 문장\n",
    "# 한번의 epoch를 진행하는 동안 한번에 8개의 데이터씩 묶어서 학습을 진행하며(batch_size=8)\n",
    "# 총 100번 epoch를 진행\n",
    "model.fit([encoder_train, decoder_train], decoder_target, batch_size=8, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"274pt\" viewBox=\"0.00 0.00 653.50 304.00\" width=\"590pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 649.5,-300 649.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2586636739592 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2586636739592</title>\n",
       "<polygon fill=\"none\" points=\"11,-249.5 11,-295.5 313,-295.5 313,-249.5 11,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-268.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"175,-249.5 175,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"175,-272.5 231,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"231,-249.5 231,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272\" y=\"-280.3\">[(?, ?, 584)]</text>\n",
       "<polyline fill=\"none\" points=\"231,-272.5 313,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272\" y=\"-257.3\">[(?, ?, 584)]</text>\n",
       "</g>\n",
       "<!-- 2585739083464 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2585739083464</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 324,-212.5 324,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68\" y=\"-185.8\">encoder: SimpleRNN</text>\n",
       "<polyline fill=\"none\" points=\"136,-166.5 136,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"136,-189.5 192,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-166.5 192,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-197.3\">(?, ?, 584)</text>\n",
       "<polyline fill=\"none\" points=\"192,-189.5 324,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-174.3\">[(?, ?, 128), (?, 128)]</text>\n",
       "</g>\n",
       "<!-- 2586636739592&#45;&gt;2585739083464 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2586636739592-&gt;2585739083464</title>\n",
       "<path d=\"M162,-249.366C162,-241.152 162,-231.658 162,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"165.5,-222.607 162,-212.607 158.5,-222.607 165.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2586627387848 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2586627387848</title>\n",
       "<polygon fill=\"none\" points=\"342.5,-166.5 342.5,-212.5 645.5,-212.5 645.5,-166.5 342.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"425\" y=\"-185.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"507.5,-166.5 507.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"535.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"507.5,-189.5 563.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"535.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"563.5,-166.5 563.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"604.5\" y=\"-197.3\">[(?, ?, 505)]</text>\n",
       "<polyline fill=\"none\" points=\"563.5,-189.5 645.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"604.5\" y=\"-174.3\">[(?, ?, 505)]</text>\n",
       "</g>\n",
       "<!-- 2586627388424 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2586627388424</title>\n",
       "<polygon fill=\"none\" points=\"165.5,-83.5 165.5,-129.5 490.5,-129.5 490.5,-83.5 165.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"234\" y=\"-102.8\">decoder: SimpleRNN</text>\n",
       "<polyline fill=\"none\" points=\"302.5,-83.5 302.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"302.5,-106.5 358.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"358.5,-83.5 358.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424.5\" y=\"-114.3\">[(?, ?, 505), (?, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"358.5,-106.5 490.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"424.5\" y=\"-91.3\">[(?, ?, 128), (?, 128)]</text>\n",
       "</g>\n",
       "<!-- 2586627387848&#45;&gt;2586627388424 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2586627387848-&gt;2586627388424</title>\n",
       "<path d=\"M448.64,-166.366C428.145,-156.366 403.765,-144.47 382.304,-133.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"383.827,-130.847 373.305,-129.607 380.758,-137.138 383.827,-130.847\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2585739083464&#45;&gt;2586627388424 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2585739083464-&gt;2586627388424</title>\n",
       "<path d=\"M207.36,-166.366C227.855,-156.366 252.235,-144.47 273.696,-133.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"275.242,-137.138 282.695,-129.607 272.173,-130.847 275.242,-137.138\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2586627367112 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2586627367112</title>\n",
       "<polygon fill=\"none\" points=\"190,-0.5 190,-46.5 466,-46.5 466,-0.5 190,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-19.8\">decoder_output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"337,-0.5 337,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"365\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"337,-23.5 393,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"365\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"393,-0.5 393,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"429.5\" y=\"-31.3\">(?, ?, 128)</text>\n",
       "<polyline fill=\"none\" points=\"393,-23.5 466,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"429.5\" y=\"-8.3\">(?, ?, 505)</text>\n",
       "</g>\n",
       "<!-- 2586627388424&#45;&gt;2586627367112 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>2586627388424-&gt;2586627367112</title>\n",
       "<path d=\"M328,-83.3664C328,-75.1516 328,-65.6579 328,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"331.5,-56.6068 328,-46.6068 324.5,-56.6069 331.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 확인\n",
    "SVG(model_to_dot(model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))\n",
    "## 이 모델에는 입력으로 항상 '인코더 인풋'과 '디코더 인풋'이 함께 들어가야 하기때문에\n",
    "## 학습에만 사용하며, 실제 사용에는 '인코더 인풋'만 들어가야 함.\n",
    "## 대신, 이미 모든 레이어의 학습은 진행되었기 때문에 weights는 학습 되어 있으므로\n",
    "## 모델의 재구성만 진행하면 사용가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 분리\n",
    "\n",
    "# 인코더 모델 생성\n",
    "encoder_model = Model(enc_inputs, enc_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"125pt\" viewBox=\"0.00 0.00 332.00 138.00\" width=\"300pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 134)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-134 328,-134 328,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2586636739592 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2586636739592</title>\n",
       "<polygon fill=\"none\" points=\"11,-83.5 11,-129.5 313,-129.5 313,-83.5 11,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"93\" y=\"-102.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"175,-83.5 175,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"175,-106.5 231,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"203\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"231,-83.5 231,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272\" y=\"-114.3\">[(?, ?, 584)]</text>\n",
       "<polyline fill=\"none\" points=\"231,-106.5 313,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"272\" y=\"-91.3\">[(?, ?, 584)]</text>\n",
       "</g>\n",
       "<!-- 2585739083464 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2585739083464</title>\n",
       "<polygon fill=\"none\" points=\"0,-0.5 0,-46.5 324,-46.5 324,-0.5 0,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"68\" y=\"-19.8\">encoder: SimpleRNN</text>\n",
       "<polyline fill=\"none\" points=\"136,-0.5 136,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"136,-23.5 192,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"164\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"192,-0.5 192,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-31.3\">(?, ?, 584)</text>\n",
       "<polyline fill=\"none\" points=\"192,-23.5 324,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"258\" y=\"-8.3\">[(?, ?, 128), (?, 128)]</text>\n",
       "</g>\n",
       "<!-- 2586636739592&#45;&gt;2585739083464 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2586636739592-&gt;2585739083464</title>\n",
       "<path d=\"M162,-83.3664C162,-75.1516 162,-65.6579 162,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"165.5,-56.6068 162,-46.6068 158.5,-56.6069 165.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코더 모델 시각화\n",
    "SVG(model_to_dot(encoder_model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 모델 생성\n",
    "\n",
    "# 디코더에서 출력되는 state를 입력하기 위한 \n",
    "# decoder_state_input이라는 새로운 인풋 레이어 생성\n",
    "# 디코더 state를 위한 레이어이므로, shape은 디코더의 state와 동일\n",
    "decoder_state_input = Input(shape=(num_units,), name='decoder_state_input')\n",
    "\n",
    "# 디코더를 생성하며, 학습에 사용하지 않았던 state를 이번에는 받음.\n",
    "# 디코더의 초기 weight에 decoder_state_input을 주어\n",
    "# 이전 SimpleRNN cell의 state를 사용\n",
    "dec_outputs, dec_state = dec(dec_inputs, initial_state=decoder_state_input)\n",
    "\n",
    "# 동일한 출력층 설계\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "# 디코더 모델 생성\n",
    "# 입력으로, [인코더의 출력] + [이전 디코더 cell의 state] 를 사용하며\n",
    "# 출력으로, [현재 디코더의 출력] + [현재 디코더의 state] 를 사용\n",
    "decoder_model = Model(\n",
    "    [dec_inputs] + [decoder_state_input],\n",
    "    [dec_outputs] + [dec_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"200pt\" viewBox=\"0.00 0.00 651.50 221.00\" width=\"588pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(0.902778 0.902778) rotate(0) translate(4 217)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-217 647.5,-217 647.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 2586627387848 -->\n",
       "<g class=\"node\" id=\"node1\"><title>2586627387848</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 303,-212.5 303,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"82.5\" y=\"-185.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"165,-166.5 165,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"165,-189.5 221,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"221,-166.5 221,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262\" y=\"-197.3\">[(?, ?, 505)]</text>\n",
       "<polyline fill=\"none\" points=\"221,-189.5 303,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262\" y=\"-174.3\">[(?, ?, 505)]</text>\n",
       "</g>\n",
       "<!-- 2586627388424 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2586627388424</title>\n",
       "<polygon fill=\"none\" points=\"154,-83.5 154,-129.5 479,-129.5 479,-83.5 154,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"222.5\" y=\"-102.8\">decoder: SimpleRNN</text>\n",
       "<polyline fill=\"none\" points=\"291,-83.5 291,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"291,-106.5 347,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"347,-83.5 347,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"413\" y=\"-114.3\">[(?, ?, 505), (?, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"347,-106.5 479,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"413\" y=\"-91.3\">[(?, ?, 128), (?, 128)]</text>\n",
       "</g>\n",
       "<!-- 2586627387848&#45;&gt;2586627388424 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2586627387848-&gt;2586627388424</title>\n",
       "<path d=\"M196.587,-166.366C216.868,-156.41 240.975,-144.576 262.237,-134.138\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"264.033,-137.155 271.468,-129.607 260.948,-130.872 264.033,-137.155\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2586648157832 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2586648157832</title>\n",
       "<polygon fill=\"none\" points=\"321.5,-166.5 321.5,-212.5 643.5,-212.5 643.5,-166.5 321.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"420.5\" y=\"-185.8\">decoder_state_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"519.5,-166.5 519.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"519.5,-189.5 575.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"547.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"575.5,-166.5 575.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"609.5\" y=\"-197.3\">[(?, 128)]</text>\n",
       "<polyline fill=\"none\" points=\"575.5,-189.5 643.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"609.5\" y=\"-174.3\">[(?, 128)]</text>\n",
       "</g>\n",
       "<!-- 2586648157832&#45;&gt;2586627388424 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>2586648157832-&gt;2586627388424</title>\n",
       "<path d=\"M437.14,-166.366C416.645,-156.366 392.265,-144.47 370.804,-133.998\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"372.327,-130.847 361.805,-129.607 369.258,-137.138 372.327,-130.847\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2586627367112 -->\n",
       "<g class=\"node\" id=\"node4\"><title>2586627367112</title>\n",
       "<polygon fill=\"none\" points=\"178.5,-0.5 178.5,-46.5 454.5,-46.5 454.5,-0.5 178.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"252\" y=\"-19.8\">decoder_output: Dense</text>\n",
       "<polyline fill=\"none\" points=\"325.5,-0.5 325.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"325.5,-23.5 381.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"353.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-0.5 381.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418\" y=\"-31.3\">(?, ?, 128)</text>\n",
       "<polyline fill=\"none\" points=\"381.5,-23.5 454.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times New Roman,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"418\" y=\"-8.3\">(?, ?, 505)</text>\n",
       "</g>\n",
       "<!-- 2586627388424&#45;&gt;2586627367112 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2586627388424-&gt;2586627367112</title>\n",
       "<path d=\"M316.5,-83.3664C316.5,-75.1516 316.5,-65.6579 316.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"320,-56.6068 316.5,-46.6068 313,-56.6069 320,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 모델 시각화\n",
    "SVG(model_to_dot(decoder_model, show_shapes=True, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델로 한글을 영어로 번역하는 함수 작성\n",
    "\n",
    "# 한글을 영어로 변역하는 함수로\n",
    "# 임의의 한글 문장을 \n",
    "# 정수 인코딩, zero-padding 및 One hot 인코딩을 진행하고,\n",
    "# 입력에 맞도록 shape만 바꿔서 입력.\n",
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    # 디코더의 초기 입력 중 두번째 입력\n",
    "    # 인코더를 사용해 한글 문장 인코딩\n",
    "    state = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # 디코더의 초기 입력 중 첫번째 입력\n",
    "    # 문장의 시작을 뜻하는 Special Symbol인 '<start>'만 추가\n",
    "    # zero 벡터를 생성하고,\n",
    "    # '<start>'를 나타내는 One hot 인코딩 생성\n",
    "    target_seq = np.zeros((1, 1, eng_num_token))\n",
    "    target_seq[0, 0, eng_words_set.index('<start>')] = 1\n",
    "    \n",
    "    # while문 반복 조건을 위한 변수 생성 및 할당\n",
    "    stop_condition = False\n",
    "    \n",
    "    # 디코딩된 영어 문장을 저장하기 위한 변수 생성\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    # stop_condition이 False인 동안 반복\n",
    "    while not stop_condition:\n",
    "        \n",
    "        # 1. 초기 입력인 문장의 시작을 알리는 '<start>'와 One hot 백터와\n",
    "        # 인코딩된 한글 문장을 디코더에 입력해 출력과 state를 받음\n",
    "        # state는 변수명이 동일하기 때문에 자동으로 업데이트...\n",
    "        # 2. 이전 디코더에서 생성된 단어의 Onehot 벡터와 업데이트된 state를\n",
    "        # 입력으로 사용해 디코더의 새로운 출력과 state를 받음, 나머지 동일하게 진행\n",
    "        output_token, state = decoder_model.predict([target_seq]+[state])\n",
    "        \n",
    "        # 출력에서 제일 큰 수가 있는 index를 받음\n",
    "        sampled_token_index = np.argmax(output_token[0, -1, :])\n",
    "        \n",
    "        # 영어 단어 집합에서 index번째의 단어를 찾음\n",
    "        sampled_word = eng_words_set[sampled_token_index]\n",
    "        \n",
    "        # 디코더 출력에서 제일 큰 수가 있는 index가 0이면 ''이기 때문에 저장하지 않음.\n",
    "        # 즉, 0이 아니면 저장.\n",
    "        if sampled_token_index != 0:\n",
    "            decoded_sentence.append(sampled_word)\n",
    "        \n",
    "        # 출력된 단어가 문장의 끝을 알리는 '<eos>'라면 \n",
    "        # 반복을 멈추기 위해 조건 변경\n",
    "        if sampled_word == '<eos>':\n",
    "            stop_condition = True\n",
    "        \n",
    "        # 현재 출력된 단어를 One hot 벡터로 생성해 \n",
    "        # 다음번 디코더의 입력으로 사용.\n",
    "        target_seq = np.zeros((1, 1, eng_num_token))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "    \n",
    "    # 디코딩된 문장에서 마지막은 <eos>가 들어있기 때문에\n",
    "    # 마지막 단어를 제외하고 반환\n",
    "    return decoded_sentence[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_sequence로 연결해 주는 중간 계층\n",
    "def translate(input_seq):\n",
    "    decode_sequences = decode_sequence(input_seq)\n",
    "    return decode_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['나는', '네가', '한국', '언어를', '배워서', '한국어로', '대화했으면', '좋겠어.']\n"
     ]
    }
   ],
   "source": [
    "# 테스트용 인코더 입력 데이터 생성 및 확인\n",
    "\n",
    "# 모델에 학습된 데이터를 사용해 한글->영어 번역 확인\n",
    "enc_sent = [kor_words_set[np.argmax(word_vec)] \n",
    "            for word_vec in encoder_train[0] if np.argmax(word_vec) != 0]\n",
    "print(len(enc_sent))\n",
    "print(enc_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['I', 'want', 'you', 'to', 'learn', 'Korean', 'and', 'talk', 'with', 'me', 'in', 'Korean.']\n"
     ]
    }
   ],
   "source": [
    "# 번역 및 결과 확인\n",
    "sentence = translate(encoder_train[0].reshape(1, kor_max_len, -1))\n",
    "print(len(sentence))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 네가 한국 언어를 배워서 한국어로 대화했으면 좋겠어.\n",
      "I want you to learn Korean and talk with me in Korean.\n"
     ]
    }
   ],
   "source": [
    "# 한글 -> 영어 결과 비교\n",
    "print(' '.join(enc_sent))\n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584\n",
      "['', '10시에', 'Ann이라고', '가', '가까이', '가는', '가능하신가요?', '가능한', '가능할까요?', '가르쳐', '가야', '가장', '가졌으면', '가족들과', '가지고', '가지씩', '가집시다.', '간다면', '감사드립니다.', '감사하겠습니다.', '감탄사를', '갔다.', '강아지와', '같아.', '같아요.', '같은', '같이', '개를', '개와', '거', '거리에', '거야.', '거에요.', '거예요.', '거의', '거짓말을', '걱정했었다.', '건', '건강한', '걸린다.', '걸어다니며', '것', '것으로', '것을', '것이', '것입니다.', '것처럼', '게임도', '결정하게', '결정했어요.', '결혼해서', '경기', '계약을', '계획하려', '고기를', '고소할', '곳까지는', '곳에', '곳을', '곳이', '공부하고', '공부하기로', '공부하는', '공부했었습니다.', '공장이', '관심에', '관심을', '관해', '광주에서', '국회', '귀국하는', '귀를', '그', '그가', '그녀가', '그는', '그대로', '그들', '그때', '그런', '그럴듯한', '그만두었습니다.', '근데', '금융업에', '기간을', '기간이', '기쁩니다.', '기억만', '깨달았습니다.', '꼭', '꽤', '꿈', '나눠', '나뉘어', '나는', '나도', '나랑', '나를', '나에게', '나였으면', '나와', '나의', '날씨가', '남산', '내', '내가', '내게', '내고', '내일', '너는', '너랑', '너무', '너의', '네,', '네가', '노력해요.', '놀랐다.', '놀이에', '눈과', '다', '다가왔어요.', '다른', '다시', '다음과', '단지', '달', '달라고', '달린', '달의', '당신', '당신들과', '당신들은', '당신의', '당신이', '대고', '대나무로', '대략', '대여하여', '대한', '대해', '대해서', '대화하는', '대화했으면', '더', '던지기를', '덜', '데려가', '도시락', '도와', '도와야', '도움과', '돈을', '돌아', '돌아가기를', '돌아갈', '돌아와서', '동료와', '동시', '동안', '동행해도', '되고', '되나요?', '되는데,', '되어', '되어서', '되었습니다.', '될', '두', '드리려고', '드립니다.', '때를', '때에만', '또한', '로그인한', '로스팅한', '리스트를', '마리가', '마을에서', '마음을', '마찬가지로', '막걸리를', '만나서', '만든', '만들어', '만들었다.', '만들자.', '만약', '많은', '많이', '말아요.', '말에', '말하려고', '말해줄', '매우', '매운', '매일', '머무는', '먹기', '먹는', '먹습니다.', '먹어야', '먹지', '메일', '메일에', '메카나', '명은', '명이', '모두', '모두에게', '모든', '목욕법을', '목적입니다.', '몸이', '못', '못하는', '무슬림이', '문서를', '문장을', '문화를', '문화에', '미국', '미국,', '미국에', '미국인과', '미는', '및', '바라요.', '바랍니다.', '박자', '받으면', '배우고', '배운다고', '배워서', '배치한', '버스', '버스를', '번', '번만', '번이라도', '번째', '번호는', '보고', '보기로', '보기를', '보내', '보내다', '보내려고', '보낸', '부족하여', '분실되었다고', '분위기의', '불필요한', '브런치를', '비해', '비행기', '비행기를', '빠져요.', '사', '사는', '사라지기를', '사람', '사람들에게', '사람들은', '사람이', '사람한테', '사무실을', '사본', '사업도', '사용한', '사용한다는', '사이트', '사진도', '살고', '살에', '살이', '샀던', '상처', '상태가', '생각합니다.', '생각해.', '생각해요.', '서류', '서울에서', '선물을', '성경에', '성지', '소개하게', '소개해', '소망해요.', '속상한', '송금이', '수', '수입이', '순례', '쉬며', '스물', '스티커', '시간', '시간을', '시간이', '시애틀에', '시작', '시험을', '식품', '싫어한다는', '심해요.', '싶니?', '싶습니다.', '싶어', '싶어.', '싶어요.', '싶은데', '싹', '아는', '아니야.', '아니에요?', '아래', '아마', '아파', '악플러들', '안', '안녕,', '안된다고', '않아서', '않아요.', '알고', '알고난', '알리고', '알아보고', '알았습니다.', '어디를', '어떻게', '어리숙한', '어제', '언니', '언어를', '언제', '언젠가는', '얼마인지', '엄마에게', '여섯', '여자들의', '열심히', '영광일', '영어로', '영어실력으로', '영업을', '오기', '오는', '오늘', '오면', '오셔야', '오픈하는', '온다면', '올린', '와서', '왔어요.', '외워', '요구했어요.', '요구했었다.', '요리를', '요즘', '우리', '우리들', '원두로', '원래', '원반', '원본', '원합니다.', '위에', '위해', '유명한', '유익한', '음식을', '음악을', '의원으로', '이', '이미', '이야기한', '이유가', '이유를', '이유만으로', '이탈리아의', '이해할', '이후', '익힌', '인도네시아에', '인수인계', '인정이', '인종', '인터넷', '인형을', '일곱', '일반', '일은', '일입니다.', '읽을', '입고', '있나요?', '있는', '있는데', '있는지', '있니?', '있다.', '있다고', '있도록', '있습니까?', '있습니다.', '있어요.', '있었다고', '자료를', '자원을', '작년', '잘', '잘못할까봐', '장비에', '저고리와', '저녁', '저녁을', '저는', '적은', '적이', '전까지는', '전에', '전통', '전통술에', '전화기', '정거장은', '정도', '정말', '제가', '제발', '제일', '조각의', '조금', '조사했습니다.', '조용한', '조율하는', '조종사라는', '종사하고', '종점까지', '좋겠어.', '좋겠어요.', '좋아하지', '좋은', '주간', '주문을', '주문한', '주문할게요.', '주세요.', '주시기', '주십시오.', '주지', '준비하고', '줄', '줄게.', '줄게요.', '중국,', '중에', '중요한', '중의', '줘야', '즐거웠기를', '즐겁기를', '즐길', '지금', '지급을', '지난', '지낸', '직원교육이', '직접', '쪽에서', '찍으며', '찍으실', '차별이', '차이점을', '차지한', '창고를', '채로', '처음', '철저한', '첨부', '첨부합니다.', '첫', '청소구역을', '청소하고', '체험해보고', '초대했습니다.', '최소', '추워요.', '축구를', '치마로', '치우친', '친구가', '친구들과', '친구들을', '친구를', '친구와', '친한', '카드', '카페가', '커피,', '커피를', '케잌을', '코멘트가', '코멘트한', '크게', '큰', '타고', '탔어야', '통째로', '파일을', '판과', '페이스북에', '포기한', '표현하지', '프로그램을', '피자', '필리핀', '필요하다고', '필요합니다.', '하고', '하나', '하는', '하는데', '하려고', '하면', '하반기를', '하지', '학생을', '한', '한가할', '한강공원에서', '한국', '한국,', '한국과', '한국보다', '한국어', '한국어로', '한국어를', '한국에', '한국에서', '한국으로', '한국의', '한국이고', '한국인들에', '한다면', '한도가', '한두', '한번', '한복도', '한복은', '한복을', '한옥', '한쪽으로', '할부로', '함께', '합니다.', '항상', '해', '해.', '해요.', '했습니다.', '했어.', '했어요.', '현금으로', '형과', '형의', '혹시', '혹시나', '확인하시길', '활동을', '회신해주시면', '회화책에', '후에,', '후원자', '후임자', '훈련', '휴대', '휴대폰을', '휴전선이', '흑인에', '희망해.', '힘들었어요.']\n"
     ]
    }
   ],
   "source": [
    "print(len(kor_words_set))\n",
    "print(kor_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.   0. 568. 533. 425. 180. 200. 161.]\n"
     ]
    }
   ],
   "source": [
    "kor_sent = '혹시 한국 전통술에 막걸리를 먹어야 되나요?'\n",
    "\n",
    "# 위에서 작성한 함수를 사용해\n",
    "# 한글 문장 정수 인코딩 및 zero-padding 진행\n",
    "enc_kor_sent = kor_encoding(kor_sent)\n",
    "print(enc_kor_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩 및 zero-padding이 진행된 한글 문장을\n",
    "# to_categorical() 함수를 사용해 One hot 인코딩 진행 및\n",
    "# enc_kor_sent는 정수 인코딩 및 zero-padding이 진행된 한글 문장이며, \n",
    "# num_classes는 One hot 인코딩을 진행할때 필요한 전체 한글 단어의 개수\n",
    "# 입력을 위해 3차원으로 shape 변경\n",
    "sentence = translate(to_categorical(enc_kor_sent, num_classes=len(kor_words_set)).reshape(1, kor_max_len, -1))\n",
    "print(len(sentence))\n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
